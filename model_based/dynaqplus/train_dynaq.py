import os
import random
import warnings
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt
from easydict import EasyDict as edict
with warnings.catch_warnings():
    warnings.simplefilter("ignore", category=DeprecationWarning)
    from torch.utils.tensorboard import SummaryWriter
import torch
from dynaqp import dynaq
os.environ['SDL_VIDEODRIVER'] = 'dummy'
os.environ['WANDB_NOTEBOOK_NAME'] = 'ddpg_with_her.py'

warnings.filterwarnings("ignore", category=DeprecationWarning)

plt.rcParams['figure.dpi'] = 100
device = torch.device("cpu")
exp = edict()

exp.exp_name = 'dynaq'  # algorithm name, in this case it should be 'DQN'
exp.env_id = 'Taxi-v3'  # name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0
exp.device = device.type  # save the device type used to load tensors and perform tensor operations
exp.max_episode_steps = 1000
exp.set_random_seed = True  # set random seed for reproducibility of python, numpy and torch
exp.seed = 2

# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)
# if the project does not exist in wandb, it will be created automatically
wandb_prj_name = f"RL_{exp.env_id}"

# name prefix of output files generated by the notebook
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"

if exp.set_random_seed:
    random.seed(exp.seed)
    np.random.seed(exp.seed)
    torch.manual_seed(exp.seed)
    torch.backends.cudnn.deterministic = exp.set_random_seed

# initialize the parameters
hypp = edict()

# flags for logging purposes
exp.enable_wandb_logging = False
exp.capture_video = True

# flags to generate agent's average performance during training
exp.eval_agent = True  # disable to speed up training
exp.eval_count = 30
exp.eval_frequency = 5000
exp.exp_type = None
# agent training specific parameters and hyperparameters
hypp.total_timesteps = 50000  # the training duration in number of time steps
hypp.alpha = 0.1 # the learning rate for the optimizer
hypp.gamma = 0.95  # decay factor of future rewards
hypp.dyna_iters = 15
hypp.epsilon =0.1 # the fraction of total_timesteps it takes to go from start_e to end_e
hypp.model_learning_rate = 0.001
hypp.dynaplus = True
hypp.time_weight = 1e-4
hypp.display_evaluation = True #display video evaluation
hypp.plot_training = True # plot training
# replay buffer parameters

# reinit run_name
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"
ddp = dynaq(exp,hypp)
ddp.train()