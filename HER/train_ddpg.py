import os

import random
import warnings
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt
from easydict import EasyDict as edict
with warnings.catch_warnings():
    warnings.simplefilter("ignore", category=DeprecationWarning)
    from torch.utils.tensorboard import SummaryWriter
import Actor as Actor
import Critic as Critic



import torch

import torch.nn.functional as F
from utils.ddpg_helper_fn import get_noisy_action,update_noisy_process,evaluate_agent
import time
from ddpg import ddpg
os.environ['SDL_VIDEODRIVER'] = 'dummy'
os.environ['WANDB_NOTEBOOK_NAME'] = 'ddpg_with_her.py'

warnings.filterwarnings("ignore", category=DeprecationWarning)

plt.rcParams['figure.dpi'] = 100
device = torch.device("cpu")
exp = edict()

exp.exp_name = 'DDPG'  # algorithm name, in this case it should be 'DQN'
exp.env_id = 'FrankaKitchen-v1'  # name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0
exp.tasks = ['microwave','kettle']
exp.terminate_on_tasks_completed = True
exp.device = device.type  # save the device type used to load tensors and perform tensor operations
exp.max_episode_steps = 1000
set_random_seed = True  # set random seed for reproducibility of python, numpy and torch
exp.seed = 2

# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)
# if the project does not exist in wandb, it will be created automatically
wandb_prj_name = f"RLLBC_{exp.env_id}"

# name prefix of output files generated by the notebook
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"

if set_random_seed:
    random.seed(exp.seed)
    np.random.seed(exp.seed)
    torch.manual_seed(exp.seed)
    torch.backends.cudnn.deterministic = set_random_seed

# initialize the parameters
hypp = edict()

# flags for logging purposes
exp.enable_wandb_logging = False
exp.capture_video = True

# flags to generate agent's average performance during training
exp.eval_agent = True  # disable to speed up training
exp.eval_count = 10
exp.eval_frequency = 5000
exp.use_HER = True # disable
hypp.train_frequency = 1
exp.goal_selection_strategy = "future"
# putting the run into the designated log folder for structuring
exp.exp_type = None  # directory the run is saved to. Should be None or a string value

# agent training specific parameters and hyperparameters
hypp.total_timesteps = 100000  # the training duration in number of time steps
hypp.learning_rate = 2e-4 # the learning rate for the optimizer
hypp.learning_rate_critic = 2e-3 # the learning rate for the optimizer
hypp.l2_reg = 0.01
hypp.gamma = 0.995  # decay factor of future rewards
hypp.buffer_size = 5000000  # the size of the replay memory buffer
hypp.target_network_frequency = 500  # the frequency of synchronizxcx16.llation with target network
hypp.batch_size = 100# number of samples taken from the replay buffer for one step
hypp.start_e = 1  # probability of exploration (epsilon) at timestep 0
hypp.end_e = 0.01  # minimal probability of exploration (epsilon)
hypp.e_steps = 30000
hypp.exploration_fraction =0.5 # the fraction of total_timesteps it takes to go from start_e to end_e
hypp.start_learning = 10000  # the timestep the learning 
hypp.tau = 0.001
hypp.k = 5
hypp.reward_scaling = 0.1
noisep = edict()
hypp.policy_delay = 10
# Define Ornstein-Uhlenbeck Process parameters
noisep.theta = 0.15
noisep.sigma = 0.2
noisep.dt = 2e-2
noisep.mu = 0
hypp.display_evaluation = True #display video evaluation
hypp.plot_training = True # plot training
# replay buffer parameters

# reinit run_name
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"
ddp = ddpg(exp,hypp,noisep)
ddp.train()