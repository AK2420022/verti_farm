import os
import copy
import time
import random
import warnings
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt
from tqdm import notebook
from easydict import EasyDict as edict
from IPython.display import Video
with warnings.catch_warnings():
    warnings.simplefilter("ignore", category=DeprecationWarning)
    from torch.utils.tensorboard import SummaryWriter
import utils.helper_fns as hf
import torch.distributions as distributions
from torch.distributions import Normal
import torch.optim.lr_scheduler as lr_scheduler
import gym
import wandb
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions.categorical import Categorical
import torch.nn.functional as F
from stable_baselines3.common.buffers import ReplayBuffer
import Actor as Actor
import EntropyNetwork as EntropyNetwork
from Critic import Critic1,Critic2
import tqdm as tqdm
import cv2
import time
os.environ['SDL_VIDEODRIVER'] = 'dummy'
os.environ['WANDB_NOTEBOOK_NAME'] = 'sac.ipynb'

warnings.filterwarnings("ignore", category=DeprecationWarning)

plt.rcParams['figure.dpi'] = 100
device = torch.device("cpu")
exp = edict()

exp.exp_name = 'SAC'  # algorithm name, in this case it should be 'DQN'
exp.env_id = 'Hopper-v3'  # name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0
exp.device = device.type  # save the device type used to load tensors and perform tensor operations

set_random_seed = True  # set random seed for reproducibility of python, numpy and torch
exp.seed = 2

# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)
# if the project does not exist in wandb, it will be created automatically
wandb_prj_name = f"RLLBC_{exp.env_id}"

# name prefix of output files generated by the notebook
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"

if set_random_seed:
    random.seed(exp.seed)
    np.random.seed(exp.seed)
    torch.manual_seed(exp.seed)
    torch.backends.cudnn.deterministic = set_random_seed

hypp = edict()

# flags for logging purposes
exp.enable_wandb_logging = True
exp.capture_video = True

# flags to generate agent's average performance during training
exp.eval_agent = True  # disable to speed up training
exp.eval_count = 10
exp.eval_frequency = 100
# putting the run into the designated log folder for structuring
exp.exp_type = None  # directory the run is saved to. Should be None or a string value

# agent training specific parameters and hyperparameters
hypp.episodes = 100000  # the training duration in number of time steps
hypp.learning_rate_actor = 5e-4# the learning rate for the optimizer
hypp.learning_rate_critic = 5e-4## the learning rate for the optimizer
hypp.learning_rate_ent = 5e-4# the learning rate for the optimizer

hypp.gamma = 0.99  # decay factor of future rewards
hypp.buffer_size = 200000  # the size of the replay memory buffer
hypp.batch_size = 256# number of samples taken from the replay buffer for one step
hypp.start_learning = 10000# the timestep the learning 
hypp.tau = 1e-2
hypp.train_frequency = 1
hypp.log_prior_type = "uniform"
hypp.deterministic = False
hypp.explore_action_steps = 5000
hypp.alpha = 1.0
hypp.hidden_dim = 256
hypp.target_entropy = -3.0
hypp.max_steps_per_episode = 200
hypp.target_update_rate = 1
hypp.entropy_init_value = 1.0
hypp.display_evaluation = True
hypp.plot_training = True
#initialize buffer
env = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(1)])

rb = ReplayBuffer(
    hypp.buffer_size,
    env.single_observation_space,
    env.single_action_space,
    device,
)

env.close()

# reinit run_name
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"

# Init tensorboard logging and wandb logging
writer = hf.setup_logging(wandb_prj_name, exp, hypp)

# create two vectorized envs: one to fill the rollout buffer with trajectories and
# another one to evaluate the agent performance at different stages of training
# Note: vectorized environments reset automatically once the episode is finished
env = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed)])
env_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(1)])

# init list to track agent's performance throughout training
tracked_returns_over_training = []
tracked_episode_len_over_training = []
tracked_episode_count = []
last_evaluated_episode = None  # stores the episode_step of when the agent's performance was last evaluated
eval_max_return = -float('inf')

# Init observation to start learning
start_time = time.time()
obs = env.reset()

pbar = tqdm.tqdm(range(1, hypp.episodes + 1))

# ------------------------- END RUN INIT --------------------------- #
# Automate entropy adjustment for Maximum Entropy RL
#adapted from https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/sac/sac.py
log_entropy = torch.zeros(1, device=device, requires_grad=True) #EntropyNetwork(env,20,1 ).to(device)
optimizer_ent_coef = optim.Adam(params = [log_entropy],lr=hypp.learning_rate_ent,weight_decay=0  )
#scheduler = lr_scheduler.LinearLR(optimizer_ent_coef, start_factor=1.0, end_factor=0.3, total_iters=50000)
# Create Actor class Instance and network optimizer pi
actor = Actor.Actor(env,hypp.hidden_dim ).to(device)
actor_target = copy.deepcopy(actor)
optimizer_actor = optim.Adam(actor.parameters(), lr=hypp.learning_rate_actor )
#scheduler = lr_scheduler.LinearLR(optimizer_actor, start_factor=1.0, end_factor=0.3, total_iters=5000)
# Create Critic class Instance and network optimizer Q1
critic = Critic1(env,hypp.hidden_dim ).to(device)
critic_target = copy.deepcopy(critic)
optimizer_critic = optim.Adam(critic.parameters() ,lr=hypp.learning_rate_critic, weight_decay=0 )
# Create Critic class Instance and network optimizer Q2
critic2 = Critic2(env,hypp.hidden_dim).to(device)
critic_target2 = copy.deepcopy(critic2)
optimizer_critic2 = optim.Adam(critic2.parameters() ,lr=hypp.learning_rate_critic, weight_decay=0 )
hypp.target_entropy = float(-np.prod(env.action_space.shape).astype(np.float32))
#init losses
ent_coef_losses, ent_coef = [],1
actor_losses, critic_losses= [],[]

#training
global_step = 0
episode_step = 0
gradient_step = 0
explore = False
log_ent = 0
#training loop
for update in pbar:

    if global_step <= hypp.explore_action_steps:
        action = env.action_space.sample()
        next_obs, reward, done, infos = env.step(action)
    else:
        action,log_prob= actor.get_action(env,torch.FloatTensor(obs),hypp.deterministic,explore)
        # apply action to environment
        next_obs, reward, done, infos = env.step(action.detach().numpy())
    global_step += 1
    # log episode return and length to tensorboard as well as current epsilon
    for info in infos:
        if "episode" in info.keys():
            episode_step += 1
            pbar.set_description(f"global_step: {global_step}, episodic_return={info['episode']['r']}")
            writer.add_scalar("rollout/episodic_return", info["episode"]["r"], global_step)
            writer.add_scalar("rollout/episodic_length", info["episode"]["l"], global_step)
            writer.add_scalar("Charts/episode_step", episode_step, global_step)
            writer.add_scalar("Charts/gradient_step", gradient_step, global_step)
            break


    # evaluation of the agent
    if exp.eval_agent and (episode_step % exp.eval_frequency == 0) and last_evaluated_episode != episode_step:
        last_evaluated_episode = episode_step
        tracked_return, tracked_episode_len = hf.evaluate_agent_sac(env_eval, actor, exp.eval_count,
                                                                exp.seed, greedy_actor=True)
        print("tracked_return: ",tracked_return )
        print("tracked_episode_len: ",tracked_episode_len )
        tracked_returns_over_training.append(tracked_return)
        tracked_episode_len_over_training.append(tracked_episode_len)
        tracked_episode_count.append([episode_step, global_step])

        # if there has been improvement of the model - save model, create video, log video to wandb
        if np.mean(tracked_return) > eval_max_return:
            eval_max_return = np.mean(tracked_return)
            # call helper function save_and_log_agent to save model, create video, log video to wandb
            hf.save_and_log_agent(exp, actor, episode_step,greedy=True, print_path=False,sac = True)

    # handling the terminal observation (vectorized env would skip terminal state)
    real_next_obs = next_obs.copy()
    for idx, d in enumerate(done):
        if d:
            real_next_obs[idx] = infos[idx]["terminal_observation"]
            next_obs = env.reset()
            
    # add data to replay buffer
    rb.add(obs, real_next_obs, torch.tensor(action).detach().numpy(), reward, done, infos)#
    # update obs
    obs = next_obs   
    # training of the agent
    if global_step > hypp.start_learning:
        if global_step % hypp.train_frequency == 0:
            #############################################################
            #sampling replay buffer
            data = rb.sample(hypp.batch_size)
            #current action and probabilites
            actions, log_prob = actor.get_action(env,data.observations.to(torch.float32),deterministic=False,explore = False)
            
            #log_prob = log_prob.reshape(-1, 1)
            ##########################################################
            with torch.autograd.set_detect_anomaly(True):

                #print("q_value_loss: ",q_value_loss.item())

                ######################################################################
                #updating critics
                with torch.no_grad():
                    new_action, new_log_prob= actor.get_action(env,data.next_observations.to(torch.float32),deterministic=False,explore = False)   
                    #train vf 
                    #new_log_prob = new_log_prob.reshape(-1, 1)
                    new_q_value1 = critic_target(data.next_observations,new_action)
                    new_q_value2 = critic_target2(data.next_observations,new_action)
                    new_q_value_min = torch.min(new_q_value1,new_q_value2)
                    new_q_value_target = new_q_value_min - ent_coef * new_log_prob
                target_q_value = data.rewards + hypp.gamma * new_q_value_target * (1-data.dones)
                # current q values
                q1 = critic(data.observations,data.actions)
                q2 = critic2(data.observations,data.actions)
                # Compute critic loss
                q_value_loss1 = 0.5 * F.mse_loss(q1,target_q_value.detach())
                q_value_loss2 = 0.5 * F.mse_loss(q2,target_q_value.detach())
                #q_value_loss = (q_value_current  - target_q_value).mean()
                #critic1
                optimizer_critic.zero_grad()
                q_value_loss1.backward(retain_graph=True)
                optimizer_critic.step()
                #critic2
                optimizer_critic2.zero_grad()
                q_value_loss2.backward(retain_graph=True)
                optimizer_critic2.step()
                
                ####################################################################
                #current_q_value1,current_q_value2 = torch.cat(critic(data.observations,data.actions), dim=1)
                current_q_value1 = critic(data.observations,data.actions)

                policy_loss = ent_coef * log_prob.mean()  - current_q_value1.mean() 
                #print("policy_loss shape: ",policy_losses.shape)
                optimizer_actor.zero_grad()
                policy_loss.backward(retain_graph=True)
                optimizer_actor.step()

                ent_coef_loss= -log_entropy * ( hypp.target_entropy + log_prob.detach().mean())
                #print("log_entropy: ",log_entropy)
                optimizer_ent_coef.zero_grad()
                ent_coef_loss.backward(retain_graph=True)
                optimizer_ent_coef.step()

                with torch.no_grad():
                    ent_coef = torch.exp(log_entropy)
                #print("actor loss: ", policy_loss)
                
                # log critic_loss and q_values to tensorboard
                if episode_step % 10 == 0:
                    #writer.add_scalar("train/critic_loss", critic_loss, global_step)
                    writer.add_scalar("train/actor_loss", policy_loss, global_step)
                    writer.add_scalar("others/SPS", int(global_step / (time.time() - start_time)), global_step)
                    writer.add_scalar("Charts/episode_step", episode_step, global_step)
                    writer.add_scalar("Charts/gradient_step", gradient_step, global_step)
                #########################################################
                # Update the frozen target models
                for param, target_param in zip(critic.parameters(), critic_target.parameters()):
                    target_param.data.copy_(hypp.tau * param.data + (1 - hypp.tau) * target_param.data)
                #update scheduler
                gradient_step+=1
                
    
# one last evaluation stage
if exp.eval_agent:
    tracked_return, tracked_episode_len = hf.evaluate_agent_sac(env_eval, actor, exp.eval_count, exp.seed, greedy_actor = True)
    tracked_returns_over_training.append(tracked_return)
    tracked_episode_len_over_training.append(tracked_episode_len)
    tracked_episode_count.append([episode_step, global_step])

    # if there has been improvement of the model - save model, create video, log video to wandb
    if np.mean(tracked_return) > eval_max_return:
        eval_max_return = np.mean(tracked_return)
        # call helper function save_and_log_agent to save model, create video, log video to wandb
        hf.save_and_log_agent(exp, actor, episode_step,greedy=True, print_path=False,sac=True)

    hf.save_tracked_values(tracked_returns_over_training, tracked_episode_len_over_training, tracked_episode_count, exp.eval_count, exp.run_name)
                
env.close()
writer.close()
pbar.close()
if wandb.run is not None:
    wandb.finish(quiet=True)
    wandb.init(mode= 'disabled')

hf.save_train_config_to_yaml(exp, hypp)    

if hypp.plot_training:
    eval_params = edict()  # eval_params - evaluation settings for trained agent

    eval_params.run_name00 = exp.run_name
    eval_params.exp_type00 = exp.exp_type

    agent_labels = []

    episode_axis_limit = None

    hf.plotter_agents_training_stats(eval_params, agent_labels, episode_axis_limit, plot_returns=True, plot_episode_len=True)
if hypp.display_evaluation:
    print("Agent")
    agent_name = exp.run_name
    agent_exp_type = exp.exp_type  # both are needed to identify the agent location
    

    exp_folder = "" if agent_exp_type is None else agent_exp_type
    filepath, _ = hf.create_folder_relative(f"{exp_folder}/{agent_name}/videos")

    hf.record_video_sac(exp.env_id, agent_name, f"{filepath}/best.mp4", exp_type=agent_exp_type, greedy=True)


    
    while True:
        #This is to check whether to break the first loop
        isclosed=0
        cap = cv2.VideoCapture(f"{filepath}/best.mp4")
        while (True):

            ret, frame = cap.read()
            # It should only show the frame when the ret is true
            if ret == True:

                cv2.imshow('frame',frame)
                if cv2.waitKey(1) == 27:
                    # When esc is pressed isclosed is 1
                    isclosed=1
                    break
            else:
                break
        time.sleep(3)
        # To break the loop if it is closed manually
        if isclosed:
            break

        cap.release()
        cv2.destroyAllWindows()